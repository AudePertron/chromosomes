{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chromos_v4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3F15kiv3T+dMjqO28HZ5I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AudePertron/chromosomes/blob/main/chromos_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrvN5tjOQiz6",
        "outputId": "877517e9-7076-439c-cbfd-5571c18b5b1c"
      },
      "source": [
        "#mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhKbA6Q1Qnkr",
        "outputId": "a7b9f039-5c10-463b-86c4-2f344a8ad945"
      },
      "source": [
        "cd gdrive/MyDrive/chromos/base_donnees/ChromSeg/region-guided\\ UNet++"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/chromos/base_donnees/ChromSeg/region-guided UNet++\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFCe9ENiqtQ2",
        "outputId": "3e12c655-f0b4-4ca6-fd5a-d578efcdf7ee"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdataset\u001b[0m/  loss.py  main.py  model.pth  \u001b[01;34m__pycache__\u001b[0m/  UNet_plus.py  utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvaA9np8RY62"
      },
      "source": [
        "#!git clone https://github.com/HKU-BAL/ChromSeg.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6KOIZpTSsBF"
      },
      "source": [
        "#!unzip /content/gdrive/MyDrive/chromos/base_donnees/ChromSeg/ChromSeg_dataset.zip -d /content/gdrive/MyDrive/chromos/base_donnees/data &> /dev/null\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noLSEbJXVsFf"
      },
      "source": [
        "#loss.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    from LovaszSoftmax.pytorch.lovasz_losses import lovasz_hinge\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "__all__ = ['BCEDiceLoss', 'LovaszHingeLoss', 'FocalLoss', 'WeightedFocalLoss']\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        bce = F.binary_cross_entropy_with_logits(input, target)\n",
        "        smooth = 1e-5\n",
        "        input = torch.sigmoid(input)\n",
        "        num = target.size(0)\n",
        "        input = input.view(num, -1)\n",
        "        target = target.view(num, -1)\n",
        "        intersection = (input * target)\n",
        "        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n",
        "        dice = 1 - dice.sum() / num\n",
        "        return 0.5 * bce + dice\n",
        "\n",
        "class LovaszHingeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = input.squeeze(1)\n",
        "        target = target.squeeze(1)\n",
        "        loss = lovasz_hinge(input, target, per_image=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=0.5, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, _input, target):\n",
        "        pt = torch.sigmoid(_input)\n",
        "        pt = pt.clamp(min=0.00001,max=0.99999)\n",
        "        loss = - self.alpha * (1 - pt) ** self.gamma * target * torch.log(pt) - \\\n",
        "            (1 - self.alpha) * pt ** self.gamma * (1 - target) * torch.log(1 - pt)\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = torch.mean(loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = torch.sum(loss)\n",
        "            \n",
        "        return loss\n",
        "\n",
        "class WeightedFocalLoss(nn.Module):\n",
        "    def __init__(self, gammas=[0, 1], alpha=0.25, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gammas = gammas\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, _input, target, weight = None):\n",
        "        pt = torch.sigmoid(_input)\n",
        "        \n",
        "        loss = torch.zeros_like(_input)\n",
        "        for gamma in self.gammas:\n",
        "            loss += - self.alpha * (1 - pt) ** gamma * target * torch.log(pt) - \\\n",
        "                (1 - self.alpha) * pt ** gamma * (1 - target) * torch.log(1 - pt)\n",
        "        \n",
        "        loss /= len(self.gammas)\n",
        "        \n",
        "        if weight is None:\n",
        "            weight = torch.ones_like(_input).to(device)\n",
        "        \n",
        "        loss *= weight\n",
        "        \n",
        "        if self.reduction == 'mean':\n",
        "            loss = torch.mean(loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = torch.sum(loss)\n",
        "            \n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuN4Egg-V-U6"
      },
      "source": [
        "#utils.py\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import PIL.Image as Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "\n",
        "'''\n",
        "The code to preprocessing and load dataset\n",
        "'''\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def make_dataset(root):\n",
        "    imgs=[]\n",
        "    for filename in os.listdir(root):\n",
        "        form = filename.split('_')[0]\n",
        "        if form == 'image':\n",
        "            tag = filename.split('_')      \n",
        "            img = os.path.join(root, filename)\n",
        "            mask1 = os.path.join(root,'binary_label_' + tag[1])\n",
        "            mask2 = os.path.join(root,'binary_label2_' + tag[1])\n",
        "            imgs.append((img,mask1,mask2))\n",
        "    return imgs\n",
        "\n",
        "class Dataset2(Dataset):\n",
        "    def __init__(self, root, transform=None, target_transform=None):\n",
        "        imgs = make_dataset(root)\n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x_path, y1_path, y2_path = self.imgs[index]\n",
        "\n",
        "        img_x = Image.open(x_path)\n",
        "        img_y1 = Image.open(y1_path)\n",
        "        img_y2 = Image.open(y2_path)\n",
        "        if self.transform is not None:\n",
        "            img_x = self.transform(img_x)\n",
        "        if self.target_transform is not None:\n",
        "            img_y1 = img_y1.convert('L')\n",
        "            img_y2 = img_y2.convert('L')\n",
        "            img_y1 = self.target_transform(img_y1)\n",
        "            img_y2 = self.target_transform(img_y2)\n",
        "        return img_x, img_y1, img_y2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "x_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "y_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def train_model(model, criterion, optimizer, dataloader_train, dataloader_val, num_epochs=20, patience=30): # use early stop\n",
        "    min_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    best_model = None\n",
        "    for epoch in range(num_epochs):\n",
        "        dt_size = len(dataloader_train.dataset)\n",
        "        # ----------------------TRAIN-----------------------\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        step = 0\n",
        "        for x, y, c in dataloader_train:\n",
        "            step += 1\n",
        "            inputs = x.to(device)\n",
        "            labels = y.to(device)\n",
        "            labels2 = c.to(device)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            loss = 0.5 * criterion(outputs[0], labels) + 0.5 * criterion(outputs[1], labels2)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            print(\"%d/%d,train_loss:%0.3f\" % (step, (dt_size - 1) // dataloader_train.batch_size + 1, loss.item()))\n",
        "        print(\"epoch %d training loss:%0.3f\" % (epoch, epoch_loss/step))\n",
        "        # ----------------------VALIDATION-----------------------\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            epoch_loss = 0\n",
        "            step = 0\n",
        "            for x, y, c in dataloader_val:\n",
        "                step += 1\n",
        "                inputs = x.to(device)\n",
        "                labels = y.to(device)\n",
        "                labels2 = c.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = 0.7 * criterion(outputs[0], labels) + 0.3 * criterion(outputs[1], labels2)\n",
        "                epoch_loss += loss.item()\n",
        "            val_loss = epoch_loss/step\n",
        "            print(\"epoch %d validation loss:%0.5f\" % (epoch, val_loss))\n",
        "        if val_loss < min_val_loss:\n",
        "            best_epoch = epoch\n",
        "            min_val_loss = val_loss\n",
        "            #torch.save(model.state_dict(), './models/weights-epoch%d-val_loss%s.pth' % (epoch, val_loss))\n",
        "            best_model = copy.deepcopy(model)\n",
        "        if epoch - best_epoch > patience:\n",
        "            break\n",
        "    print('Best validation loss%0.5f at epoch%s'% (min_val_loss, best_epoch))\n",
        "    return best_model\n",
        "\n",
        "def meanIOU_per_image(y_pred, y_true):\n",
        "    '''\n",
        "    Calculate the IOU, averaged across images\n",
        "    '''\n",
        "    y_pred = y_pred.astype('bool')\n",
        "    y_true = y_true.astype('bool')\n",
        "    intersection = np.logical_and(y_true, y_pred)\n",
        "    union = np.logical_or(y_true, y_pred)\n",
        "    \n",
        "    return np.sum(intersection) / np.sum(union)\n",
        "\n",
        "class Score():\n",
        "    def __init__(self, y_pred, y_true, size = 512, threshold = 0.5):\n",
        "        self.TN = 0\n",
        "        self.FN = 0\n",
        "        self.FP = 0\n",
        "        self.TP = 0\n",
        "        self.y_pred = y_pred > threshold\n",
        "        self.y_true = y_true\n",
        "        self.threshold = threshold\n",
        "        \n",
        "        for i in range(0, size):\n",
        "            for j in range(0, size):\n",
        "                if self.y_pred[i,j] == 1:\n",
        "                    if self.y_pred[i,j] == self.y_true[i][j]:\n",
        "                        self.TP = self.TP + 1\n",
        "                    else:\n",
        "                        self.FP = self.FP + 1\n",
        "                else:\n",
        "                    if self.y_pred[i,j] == self.y_true[i][j]:\n",
        "                        self.TN = self.TN + 1\n",
        "                    else:\n",
        "                        self.FN = self.FN + 1        \n",
        " \n",
        "    def get_Se(self):\n",
        "        return (self.TP)/(self.TP + self.FN)\n",
        "    \n",
        "    def get_Sp(self):\n",
        "        return (self.TN)/(self.TN + self.FP)\n",
        "    \n",
        "    def get_Pr(self):\n",
        "        return (self.TP)/(self.TP + self.FP)\n",
        "    \n",
        "    def F1(self):\n",
        "        Pr = self.get_Pr()\n",
        "        Se = self.get_Se()\n",
        "        return (2*Pr*Se)/(Pr + Se)\n",
        "    \n",
        "    def G(self):\n",
        "        Sp = self.get_Sp()\n",
        "        Se = self.get_Se()\n",
        "        return math.sqrt(Se*Sp)\n",
        "    \n",
        "    def IoU(self):\n",
        "        Pr = self.get_Pr()\n",
        "        Se = self.get_Se()\n",
        "        return (Pr*Se) /(Pr + Se - Pr*Se)\n",
        "    \n",
        "    def DSC(self):\n",
        "        return (2* self.TP)/(2* self.TP + self.FP + self.FN) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNGGwDFoWJlO"
      },
      "source": [
        "#Unet_plus.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    # The convolutional layer: conv3-relu-conv3-relu\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(mid_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "\n",
        "class UNet_plus2(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_ch=3, out_ch=1):\n",
        "        super(UNet_plus2, self).__init__()\n",
        "        self.n_channels = in_ch\n",
        "        self.n_classes = out_ch\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8]\n",
        "\n",
        "        # Encoder: U-Net\n",
        "        #self.conv0_0 = nn.Sequential(\n",
        "        #    nn.Conv2d(in_ch, filters[0], kernel_size=7, stride=1, padding=3,\n",
        "        #         bias=False),\n",
        "        #    nn.BatchNorm2d(filters[0]),\n",
        "        #    nn.ReLU(inplace=True)\n",
        "        #)\n",
        "        self.conv0_0 = DoubleConv(self.n_channels, filters[0], filters[0])\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
        "        self.conv1_0= DoubleConv(filters[0], filters[1], filters[1])\n",
        "        self.conv2_0= DoubleConv(filters[1], filters[2], filters[2])\n",
        "        self.conv3_0= DoubleConv(filters[2], filters[3], filters[3])\n",
        "\n",
        "        # Upsample layer(Deconv)\n",
        "        self.up1_0 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)\n",
        "        self.up2_0 = nn.ConvTranspose2d(filters[2], filters[1], 2, stride=2)\n",
        "        self.up3_0 = nn.ConvTranspose2d(filters[3], filters[2], 2, stride=2)\n",
        "        self.up1_1 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)\n",
        "        self.up2_1 = nn.ConvTranspose2d(filters[2], filters[1], 2, stride=2)\n",
        "        self.up1_2 = nn.ConvTranspose2d(filters[1], filters[0], 2, stride=2)\n",
        "\n",
        "        # Mid Layer\n",
        "        self.conv0_1 = DoubleConv(filters[0]*2, filters[0], filters[0]) \n",
        "        self.conv1_1 = DoubleConv(filters[1]*2, filters[1], filters[1])\n",
        "        self.conv2_1 = DoubleConv(filters[2]*2, filters[2], filters[2])\n",
        "\n",
        "        self.conv0_2 = DoubleConv(filters[0]*2, filters[0], filters[0])\n",
        "        self.conv1_2 = DoubleConv(filters[1]*2, filters[1], filters[1])\n",
        "\n",
        "        self.conv0_3 = DoubleConv(filters[0]*2, filters[0], filters[0])\n",
        "\n",
        "        # attention\n",
        "        # self.attention0 = ContourAttention(filters[0])\n",
        "        # self.attention1 = ContourAttention(filters[1])\n",
        "        \n",
        "        self.contour = nn.Sequential(\n",
        "            nn.Conv2d(filters[0] * 2, filters[0], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0], out_ch, 1)\n",
        "        )\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(filters[0], out_ch, 1)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0_0 = self.conv0_0(x)\n",
        "        p = self.pool(x0_0)\n",
        "        x1_0 = self.conv1_0(p)\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up1_0(x1_0)], 1)) + x0_0\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up2_0(x2_0)], 1))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up3_0(x3_0)], 1))\n",
        "        \n",
        "        x0_2 = self.conv0_2(torch.cat([x0_1, self.up1_1(x1_1)], 1)) + x0_1\n",
        "        # x1_2 = self.conv1_2(torch.cat([self.attention1(x1_1, x1_0), self.up2_1(x2_1)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, self.up2_1(x2_1)], 1))\n",
        "        \n",
        "        # x0_3 = self.conv0_3(torch.cat([self.attention0(x0_2, x0_0), self.up1_2(x1_2)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, self.up1_2(x1_2)], 1))\n",
        "        \n",
        "        contour = self.contour(torch.cat([x0_1, x0_2], dim=1))\n",
        "        output = self.final(x0_3)\n",
        "\n",
        "        return self.sigmoid(output), self.sigmoid(contour)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9TVTMOsLeEn",
        "outputId": "85b7af30-c5b6-4039-b7aa-754774b065e0"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 7060113071745259719, name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11345264640\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 1816299314727320445\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MK_L5KYWTJ3",
        "outputId": "06c407a2-4c34-4b6b-fec5-a47b867429be"
      },
      "source": [
        "#main.py\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.models as models\n",
        "import PIL.Image as Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import copy\n",
        "#from loss import *\n",
        "#from UNet_plus import UNet_plus2 as UNet_plus2\n",
        "#from utils import *\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    IMAGE_SIZE = 256\n",
        "    batch_size = 8\n",
        "    \n",
        "    torch.manual_seed(0)    # reproducible\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"device : \", device)\n",
        "    model = UNet_plus2(3, 1)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    gamma=1\n",
        "    alpha=0.75\n",
        "    criterion = FocalLoss(gamma, alpha, reduction='mean')\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    dataset_train = Dataset2('dataset/train', transform=x_transforms, target_transform=y_transforms)\n",
        "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    dataset_val = Dataset2('dataset/val', transform=x_transforms, target_transform=y_transforms)\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    \n",
        "    model = train_model(model, criterion, optimizer, dataloader_train, dataloader_val, num_epochs=100, patience=7)\n",
        "    \n",
        "    model = model.cpu()\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=True, num_workers=0)\n",
        "\n",
        "    iou = 0\n",
        "    iou2 = 0\n",
        "    n = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, target, c in dataloader_val:\n",
        "            n = n + 1\n",
        "            y = model(x)\n",
        "            y_pred_0 = torch.squeeze(y[0]).numpy()\n",
        "            y_pred = np.zeros((256,256))\n",
        "            y_pred[y_pred_0 > 0.5] = 1.0\n",
        "            y_2_0 = torch.squeeze(y[1]).numpy()\n",
        "            y_2 = np.zeros((256,256))\n",
        "            y_2[y_2_0 > 0.5] = 1.0\n",
        "            y_true = torch.squeeze(target).numpy()\n",
        "            y_true2 = torch.squeeze(c).numpy()\n",
        "            output1 = np.reshape(y_pred * 255,(256,256))\n",
        "            output2 = np.reshape(y_2 * 255,(256,256))\n",
        "\n",
        "            x_image = torch.squeeze(x).numpy()\n",
        "            image = np.dstack((x_image[0,...]*255, x_image[1,...]*255, x_image[2,...]*255))\n",
        "\n",
        "            cv2.imwrite('output1/' + str(n) + \".png\", output1)\n",
        "            cv2.imwrite('output2/' + str(n) + \".png\", output2)\n",
        "            cv2.imwrite('img/' + str(n) + \".png\", image)\n",
        "            try:\n",
        "                iou += meanIOU_per_image(y_pred, y_true)\n",
        "                iou2 += meanIOU_per_image(y_2, y_true2)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print('y_pred: %s'% y_pred)\n",
        "                print('y_true: %s'% y_true)\n",
        "                torch.save(model.state_dict(), './models/%s_seed%s_error.pth' % ('cach√©', 'coucou'))\n",
        "                sys.exit(0)\n",
        "    IoU = float(iou/n)\n",
        "    IoU2 = float(iou2/n)\n",
        "    \n",
        "    print('Final_IoU: %s'% IoU)\n",
        "    print('Final_IoU2: %s'% IoU2)\n",
        "    torch.save(model.state_dict(), 'model.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device :  cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/32,train_loss:0.154\n",
            "2/32,train_loss:0.144\n",
            "3/32,train_loss:0.141\n",
            "4/32,train_loss:0.137\n",
            "5/32,train_loss:0.132\n",
            "6/32,train_loss:0.132\n",
            "7/32,train_loss:0.128\n",
            "8/32,train_loss:0.126\n",
            "9/32,train_loss:0.124\n",
            "10/32,train_loss:0.121\n",
            "11/32,train_loss:0.120\n",
            "12/32,train_loss:0.119\n",
            "13/32,train_loss:0.119\n",
            "14/32,train_loss:0.117\n",
            "15/32,train_loss:0.117\n",
            "16/32,train_loss:0.116\n",
            "17/32,train_loss:0.116\n",
            "18/32,train_loss:0.115\n",
            "19/32,train_loss:0.114\n",
            "20/32,train_loss:0.114\n",
            "21/32,train_loss:0.113\n",
            "22/32,train_loss:0.112\n",
            "23/32,train_loss:0.112\n",
            "24/32,train_loss:0.111\n",
            "25/32,train_loss:0.111\n",
            "26/32,train_loss:0.111\n",
            "27/32,train_loss:0.110\n",
            "28/32,train_loss:0.110\n",
            "29/32,train_loss:0.109\n",
            "30/32,train_loss:0.109\n",
            "31/32,train_loss:0.109\n",
            "32/32,train_loss:0.108\n",
            "epoch 0 training loss:0.120\n",
            "epoch 0 validation loss:0.10722\n",
            "1/32,train_loss:0.108\n",
            "2/32,train_loss:0.108\n",
            "3/32,train_loss:0.107\n",
            "4/32,train_loss:0.107\n",
            "5/32,train_loss:0.107\n",
            "6/32,train_loss:0.106\n",
            "7/32,train_loss:0.106\n",
            "8/32,train_loss:0.106\n",
            "9/32,train_loss:0.105\n",
            "10/32,train_loss:0.105\n",
            "11/32,train_loss:0.105\n",
            "12/32,train_loss:0.104\n",
            "13/32,train_loss:0.104\n",
            "14/32,train_loss:0.104\n",
            "15/32,train_loss:0.104\n",
            "16/32,train_loss:0.103\n",
            "17/32,train_loss:0.103\n",
            "18/32,train_loss:0.103\n",
            "19/32,train_loss:0.103\n",
            "20/32,train_loss:0.102\n",
            "21/32,train_loss:0.102\n",
            "22/32,train_loss:0.102\n",
            "23/32,train_loss:0.101\n",
            "24/32,train_loss:0.101\n",
            "25/32,train_loss:0.101\n",
            "26/32,train_loss:0.101\n",
            "27/32,train_loss:0.100\n",
            "28/32,train_loss:0.100\n",
            "29/32,train_loss:0.100\n",
            "30/32,train_loss:0.100\n",
            "31/32,train_loss:0.100\n",
            "32/32,train_loss:0.099\n",
            "epoch 1 training loss:0.103\n",
            "epoch 1 validation loss:0.12334\n",
            "1/32,train_loss:0.099\n",
            "2/32,train_loss:0.099\n",
            "3/32,train_loss:0.099\n",
            "4/32,train_loss:0.098\n",
            "5/32,train_loss:0.098\n",
            "6/32,train_loss:0.098\n",
            "7/32,train_loss:0.098\n",
            "8/32,train_loss:0.098\n",
            "9/32,train_loss:0.098\n",
            "10/32,train_loss:0.097\n",
            "11/32,train_loss:0.097\n",
            "12/32,train_loss:0.097\n",
            "13/32,train_loss:0.097\n",
            "14/32,train_loss:0.097\n",
            "15/32,train_loss:0.097\n",
            "16/32,train_loss:0.096\n",
            "17/32,train_loss:0.096\n",
            "18/32,train_loss:0.096\n",
            "19/32,train_loss:0.096\n",
            "20/32,train_loss:0.096\n",
            "21/32,train_loss:0.096\n",
            "22/32,train_loss:0.096\n",
            "23/32,train_loss:0.095\n",
            "24/32,train_loss:0.095\n",
            "25/32,train_loss:0.095\n",
            "26/32,train_loss:0.095\n",
            "27/32,train_loss:0.095\n",
            "28/32,train_loss:0.095\n",
            "29/32,train_loss:0.094\n",
            "30/32,train_loss:0.095\n",
            "31/32,train_loss:0.094\n",
            "32/32,train_loss:0.094\n",
            "epoch 2 training loss:0.096\n",
            "epoch 2 validation loss:0.09465\n",
            "1/32,train_loss:0.094\n",
            "2/32,train_loss:0.094\n",
            "3/32,train_loss:0.094\n",
            "4/32,train_loss:0.094\n",
            "5/32,train_loss:0.094\n",
            "6/32,train_loss:0.094\n",
            "7/32,train_loss:0.094\n",
            "8/32,train_loss:0.094\n",
            "9/32,train_loss:0.093\n",
            "10/32,train_loss:0.093\n",
            "11/32,train_loss:0.093\n",
            "12/32,train_loss:0.093\n",
            "13/32,train_loss:0.093\n",
            "14/32,train_loss:0.093\n",
            "15/32,train_loss:0.093\n",
            "16/32,train_loss:0.093\n",
            "17/32,train_loss:0.093\n",
            "18/32,train_loss:0.092\n",
            "19/32,train_loss:0.093\n",
            "20/32,train_loss:0.093\n",
            "21/32,train_loss:0.093\n",
            "22/32,train_loss:0.092\n",
            "23/32,train_loss:0.092\n",
            "24/32,train_loss:0.092\n",
            "25/32,train_loss:0.092\n",
            "26/32,train_loss:0.092\n",
            "27/32,train_loss:0.092\n",
            "28/32,train_loss:0.092\n",
            "29/32,train_loss:0.092\n",
            "30/32,train_loss:0.092\n",
            "31/32,train_loss:0.092\n",
            "32/32,train_loss:0.092\n",
            "epoch 3 training loss:0.093\n",
            "epoch 3 validation loss:0.09144\n",
            "1/32,train_loss:0.091\n",
            "2/32,train_loss:0.091\n",
            "3/32,train_loss:0.091\n",
            "4/32,train_loss:0.091\n",
            "5/32,train_loss:0.091\n",
            "6/32,train_loss:0.091\n",
            "7/32,train_loss:0.091\n",
            "8/32,train_loss:0.091\n",
            "9/32,train_loss:0.091\n",
            "10/32,train_loss:0.091\n",
            "11/32,train_loss:0.091\n",
            "12/32,train_loss:0.091\n",
            "13/32,train_loss:0.091\n",
            "14/32,train_loss:0.091\n",
            "15/32,train_loss:0.091\n",
            "16/32,train_loss:0.091\n",
            "17/32,train_loss:0.091\n",
            "18/32,train_loss:0.091\n",
            "19/32,train_loss:0.091\n",
            "20/32,train_loss:0.091\n",
            "21/32,train_loss:0.090\n",
            "22/32,train_loss:0.090\n",
            "23/32,train_loss:0.090\n",
            "24/32,train_loss:0.090\n",
            "25/32,train_loss:0.090\n",
            "26/32,train_loss:0.090\n",
            "27/32,train_loss:0.090\n",
            "28/32,train_loss:0.090\n",
            "29/32,train_loss:0.090\n",
            "30/32,train_loss:0.090\n",
            "31/32,train_loss:0.090\n",
            "32/32,train_loss:0.090\n",
            "epoch 4 training loss:0.091\n",
            "epoch 4 validation loss:0.10135\n",
            "1/32,train_loss:0.090\n",
            "2/32,train_loss:0.090\n",
            "3/32,train_loss:0.090\n",
            "4/32,train_loss:0.090\n",
            "5/32,train_loss:0.090\n",
            "6/32,train_loss:0.090\n",
            "7/32,train_loss:0.090\n",
            "8/32,train_loss:0.090\n",
            "9/32,train_loss:0.090\n",
            "10/32,train_loss:0.090\n",
            "11/32,train_loss:0.090\n",
            "12/32,train_loss:0.090\n",
            "13/32,train_loss:0.090\n",
            "14/32,train_loss:0.090\n",
            "15/32,train_loss:0.090\n",
            "16/32,train_loss:0.090\n",
            "17/32,train_loss:0.090\n",
            "18/32,train_loss:0.090\n",
            "19/32,train_loss:0.089\n",
            "20/32,train_loss:0.089\n",
            "21/32,train_loss:0.089\n",
            "22/32,train_loss:0.089\n",
            "23/32,train_loss:0.089\n",
            "24/32,train_loss:0.089\n",
            "25/32,train_loss:0.089\n",
            "26/32,train_loss:0.089\n",
            "27/32,train_loss:0.089\n",
            "28/32,train_loss:0.089\n",
            "29/32,train_loss:0.089\n",
            "30/32,train_loss:0.089\n",
            "31/32,train_loss:0.089\n",
            "32/32,train_loss:0.089\n",
            "epoch 5 training loss:0.089\n",
            "epoch 5 validation loss:0.09009\n",
            "1/32,train_loss:0.089\n",
            "2/32,train_loss:0.089\n",
            "3/32,train_loss:0.089\n",
            "4/32,train_loss:0.089\n",
            "5/32,train_loss:0.089\n",
            "6/32,train_loss:0.089\n",
            "7/32,train_loss:0.089\n",
            "8/32,train_loss:0.089\n",
            "9/32,train_loss:0.089\n",
            "10/32,train_loss:0.089\n",
            "11/32,train_loss:0.089\n",
            "12/32,train_loss:0.089\n",
            "13/32,train_loss:0.089\n",
            "14/32,train_loss:0.089\n",
            "15/32,train_loss:0.089\n",
            "16/32,train_loss:0.088\n",
            "17/32,train_loss:0.089\n",
            "18/32,train_loss:0.088\n",
            "19/32,train_loss:0.088\n",
            "20/32,train_loss:0.089\n",
            "21/32,train_loss:0.088\n",
            "22/32,train_loss:0.088\n",
            "23/32,train_loss:0.089\n",
            "24/32,train_loss:0.088\n",
            "25/32,train_loss:0.088\n",
            "26/32,train_loss:0.088\n",
            "27/32,train_loss:0.088\n",
            "28/32,train_loss:0.088\n",
            "29/32,train_loss:0.088\n",
            "30/32,train_loss:0.088\n",
            "31/32,train_loss:0.088\n",
            "32/32,train_loss:0.088\n",
            "epoch 6 training loss:0.089\n",
            "epoch 6 validation loss:0.08834\n",
            "1/32,train_loss:0.088\n",
            "2/32,train_loss:0.088\n",
            "3/32,train_loss:0.088\n",
            "4/32,train_loss:0.088\n",
            "5/32,train_loss:0.088\n",
            "6/32,train_loss:0.088\n",
            "7/32,train_loss:0.088\n",
            "8/32,train_loss:0.088\n",
            "9/32,train_loss:0.088\n",
            "10/32,train_loss:0.088\n",
            "11/32,train_loss:0.088\n",
            "12/32,train_loss:0.088\n",
            "13/32,train_loss:0.088\n",
            "14/32,train_loss:0.088\n",
            "15/32,train_loss:0.088\n",
            "16/32,train_loss:0.088\n",
            "17/32,train_loss:0.088\n",
            "18/32,train_loss:0.088\n",
            "19/32,train_loss:0.088\n",
            "20/32,train_loss:0.088\n",
            "21/32,train_loss:0.088\n",
            "22/32,train_loss:0.088\n",
            "23/32,train_loss:0.088\n",
            "24/32,train_loss:0.088\n",
            "25/32,train_loss:0.088\n",
            "26/32,train_loss:0.088\n",
            "27/32,train_loss:0.088\n",
            "28/32,train_loss:0.088\n",
            "29/32,train_loss:0.088\n",
            "30/32,train_loss:0.088\n",
            "31/32,train_loss:0.088\n",
            "32/32,train_loss:0.088\n",
            "epoch 7 training loss:0.088\n",
            "epoch 7 validation loss:0.08822\n",
            "1/32,train_loss:0.088\n",
            "2/32,train_loss:0.088\n",
            "3/32,train_loss:0.088\n",
            "4/32,train_loss:0.088\n",
            "5/32,train_loss:0.088\n",
            "6/32,train_loss:0.088\n",
            "7/32,train_loss:0.088\n",
            "8/32,train_loss:0.088\n",
            "9/32,train_loss:0.088\n",
            "10/32,train_loss:0.088\n",
            "11/32,train_loss:0.088\n",
            "12/32,train_loss:0.088\n",
            "13/32,train_loss:0.088\n",
            "14/32,train_loss:0.088\n",
            "15/32,train_loss:0.088\n",
            "16/32,train_loss:0.088\n",
            "17/32,train_loss:0.088\n",
            "18/32,train_loss:0.088\n",
            "19/32,train_loss:0.088\n",
            "20/32,train_loss:0.088\n",
            "21/32,train_loss:0.088\n",
            "22/32,train_loss:0.088\n",
            "23/32,train_loss:0.088\n",
            "24/32,train_loss:0.088\n",
            "25/32,train_loss:0.088\n",
            "26/32,train_loss:0.088\n",
            "27/32,train_loss:0.088\n",
            "28/32,train_loss:0.088\n",
            "29/32,train_loss:0.088\n",
            "30/32,train_loss:0.088\n",
            "31/32,train_loss:0.088\n",
            "32/32,train_loss:0.088\n",
            "epoch 8 training loss:0.088\n",
            "epoch 8 validation loss:0.08882\n",
            "1/32,train_loss:0.088\n",
            "2/32,train_loss:0.088\n",
            "3/32,train_loss:0.088\n",
            "4/32,train_loss:0.088\n",
            "5/32,train_loss:0.088\n",
            "6/32,train_loss:0.088\n",
            "7/32,train_loss:0.088\n",
            "8/32,train_loss:0.088\n",
            "9/32,train_loss:0.088\n",
            "10/32,train_loss:0.088\n",
            "11/32,train_loss:0.088\n",
            "12/32,train_loss:0.088\n",
            "13/32,train_loss:0.088\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.088\n",
            "16/32,train_loss:0.088\n",
            "17/32,train_loss:0.088\n",
            "18/32,train_loss:0.088\n",
            "19/32,train_loss:0.088\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.088\n",
            "22/32,train_loss:0.088\n",
            "23/32,train_loss:0.088\n",
            "24/32,train_loss:0.088\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 9 training loss:0.088\n",
            "epoch 9 validation loss:0.08771\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.088\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 10 training loss:0.087\n",
            "epoch 10 validation loss:0.08949\n",
            "1/32,train_loss:0.088\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 11 training loss:0.087\n",
            "epoch 11 validation loss:0.12632\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 12 training loss:0.087\n",
            "epoch 12 validation loss:0.08740\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 13 training loss:0.087\n",
            "epoch 13 validation loss:0.08718\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 14 training loss:0.087\n",
            "epoch 14 validation loss:0.08701\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 15 training loss:0.087\n",
            "epoch 15 validation loss:0.08701\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 16 training loss:0.087\n",
            "epoch 16 validation loss:0.08719\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.086\n",
            "epoch 17 training loss:0.087\n",
            "epoch 17 validation loss:0.08741\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 18 training loss:0.087\n",
            "epoch 18 validation loss:0.08709\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 19 training loss:0.087\n",
            "epoch 19 validation loss:0.08683\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 20 training loss:0.087\n",
            "epoch 20 validation loss:0.08705\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 21 training loss:0.087\n",
            "epoch 21 validation loss:0.08680\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.087\n",
            "epoch 22 training loss:0.087\n",
            "epoch 22 validation loss:0.08677\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.087\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.087\n",
            "epoch 23 training loss:0.087\n",
            "epoch 23 validation loss:0.08678\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.087\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.087\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 24 training loss:0.087\n",
            "epoch 24 validation loss:0.08669\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.087\n",
            "epoch 25 training loss:0.086\n",
            "epoch 25 validation loss:0.08673\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.087\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.087\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.087\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.087\n",
            "epoch 26 training loss:0.086\n",
            "epoch 26 validation loss:0.08678\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.087\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.087\n",
            "32/32,train_loss:0.086\n",
            "epoch 27 training loss:0.086\n",
            "epoch 27 validation loss:0.08680\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.087\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.087\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.087\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.087\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 28 training loss:0.086\n",
            "epoch 28 validation loss:0.08675\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.087\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 29 training loss:0.086\n",
            "epoch 29 validation loss:0.08666\n",
            "1/32,train_loss:0.087\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.087\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.087\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 30 training loss:0.086\n",
            "epoch 30 validation loss:0.08661\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 31 training loss:0.086\n",
            "epoch 31 validation loss:0.08666\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.087\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 32 training loss:0.086\n",
            "epoch 32 validation loss:0.08678\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.087\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.087\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 33 training loss:0.086\n",
            "epoch 33 validation loss:0.08661\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.087\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.087\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 34 training loss:0.086\n",
            "epoch 34 validation loss:0.12690\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.087\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 35 training loss:0.086\n",
            "epoch 35 validation loss:0.08660\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.087\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 36 training loss:0.086\n",
            "epoch 36 validation loss:0.08672\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 37 training loss:0.086\n",
            "epoch 37 validation loss:0.08658\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.087\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 38 training loss:0.086\n",
            "epoch 38 validation loss:0.08658\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.087\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 39 training loss:0.086\n",
            "epoch 39 validation loss:0.08665\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.087\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 40 training loss:0.086\n",
            "epoch 40 validation loss:0.08735\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.087\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.087\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.087\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.087\n",
            "epoch 41 training loss:0.086\n",
            "epoch 41 validation loss:0.08668\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.087\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 42 training loss:0.086\n",
            "epoch 42 validation loss:0.08662\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 43 training loss:0.086\n",
            "epoch 43 validation loss:0.08655\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 44 training loss:0.086\n",
            "epoch 44 validation loss:0.08659\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 45 training loss:0.086\n",
            "epoch 45 validation loss:0.08655\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.087\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 46 training loss:0.086\n",
            "epoch 46 validation loss:0.08661\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 47 training loss:0.086\n",
            "epoch 47 validation loss:0.08657\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 48 training loss:0.086\n",
            "epoch 48 validation loss:0.08668\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 49 training loss:0.086\n",
            "epoch 49 validation loss:0.08666\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 50 training loss:0.086\n",
            "epoch 50 validation loss:0.08655\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 51 training loss:0.086\n",
            "epoch 51 validation loss:0.08657\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 52 training loss:0.086\n",
            "epoch 52 validation loss:0.08652\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 53 training loss:0.086\n",
            "epoch 53 validation loss:0.08656\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 54 training loss:0.086\n",
            "epoch 54 validation loss:0.08656\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 55 training loss:0.086\n",
            "epoch 55 validation loss:0.08655\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 56 training loss:0.086\n",
            "epoch 56 validation loss:0.08667\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 57 training loss:0.086\n",
            "epoch 57 validation loss:0.08657\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 58 training loss:0.086\n",
            "epoch 58 validation loss:0.08653\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 59 training loss:0.086\n",
            "epoch 59 validation loss:0.08655\n",
            "1/32,train_loss:0.086\n",
            "2/32,train_loss:0.086\n",
            "3/32,train_loss:0.086\n",
            "4/32,train_loss:0.086\n",
            "5/32,train_loss:0.086\n",
            "6/32,train_loss:0.086\n",
            "7/32,train_loss:0.086\n",
            "8/32,train_loss:0.086\n",
            "9/32,train_loss:0.086\n",
            "10/32,train_loss:0.086\n",
            "11/32,train_loss:0.086\n",
            "12/32,train_loss:0.086\n",
            "13/32,train_loss:0.086\n",
            "14/32,train_loss:0.086\n",
            "15/32,train_loss:0.086\n",
            "16/32,train_loss:0.086\n",
            "17/32,train_loss:0.086\n",
            "18/32,train_loss:0.086\n",
            "19/32,train_loss:0.086\n",
            "20/32,train_loss:0.086\n",
            "21/32,train_loss:0.086\n",
            "22/32,train_loss:0.086\n",
            "23/32,train_loss:0.086\n",
            "24/32,train_loss:0.086\n",
            "25/32,train_loss:0.086\n",
            "26/32,train_loss:0.086\n",
            "27/32,train_loss:0.086\n",
            "28/32,train_loss:0.086\n",
            "29/32,train_loss:0.086\n",
            "30/32,train_loss:0.086\n",
            "31/32,train_loss:0.086\n",
            "32/32,train_loss:0.086\n",
            "epoch 60 training loss:0.086\n",
            "epoch 60 validation loss:0.08656\n",
            "Best validation loss0.08652 at epoch52\n",
            "Final_IoU: 0.7887080119047074\n",
            "Final_IoU2: 0.9591130444522612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nooJXq52mdqX",
        "outputId": "90d55c1f-1d3b-44d0-91f6-d8d49bfa0226"
      },
      "source": [
        "#model = model = UNet_plus2(3, 1)\n",
        "#model.load_state_dict(torch.load('/content/gdrive/MyDrive/chromos/base_donnees/ChromSeg/region-guided UNet++/model.pth'))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet_plus2(\n",
              "  (conv0_0): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "  (conv1_0): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv2_0): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv3_0): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (up1_0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up2_0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up3_0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up1_1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up2_1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (up1_2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (conv0_1): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv1_1): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv2_1): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv0_2): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv1_2): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv0_3): DoubleConv(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (contour): Sequential(\n",
              "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (final): Sequential(\n",
              "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn2dSHp8RmRv"
      },
      "source": [
        "image = Image.open('/content/gdrive/MyDrive/chromos/base_donnees/ChromSeg/crossing-partition/img/2.png')\n",
        "\n",
        "x_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "image_test = x_transforms(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ARlbdgXpwkg"
      },
      "source": [
        "pred_test = model(image_test[None, ...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQcVx6F_uva3"
      },
      "source": [
        "n = 0\n",
        "with torch.no_grad():\n",
        "  y = model(image_test[None, ...])\n",
        "  y_pred_0 = torch.squeeze(y[0]).numpy()\n",
        "  y_pred = np.zeros((256,256))\n",
        "  y_pred[y_pred_0 > 0.5] = 1.0\n",
        "  y_2_0 = torch.squeeze(y[1]).numpy()\n",
        "  y_2 = np.zeros((256,256))\n",
        "  y_2[y_2_0 > 0.5] = 1.0\n",
        "  output1 = np.reshape(y_pred * 255,(256,256))\n",
        "  output2 = np.reshape(y_2 * 255,(256,256))\n",
        "\n",
        "  x_image = torch.squeeze(image_test).numpy()\n",
        "  image = np.dstack((x_image[0,...]*255, x_image[1,...]*255, x_image[2,...]*255))\n",
        "\n",
        "  cv2.imwrite('/content/gdrive/MyDrive/chromos/base_donnees/outputs/output1/' + str(n) + \".png\", output1)\n",
        "  cv2.imwrite('/content/gdrive/MyDrive/chromos/base_donnees/outputs/output2/' + str(n) + \".png\", output2)\n",
        "  cv2.imwrite('/content/gdrive/MyDrive/chromos/base_donnees/outputs/images/' + str(n) + \".png\", image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ned2whsHuTK-",
        "outputId": "7786d49e-505c-4ddc-d0e6-98295aaf4ec2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(output1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc3a0615e90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "AqDPsrgj20-l",
        "outputId": "0186538f-593c-47e0-b1d0-2e643112d7c7"
      },
      "source": [
        "plt.imshow(output2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc39ebe70d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "HyaC5YqF23QH",
        "outputId": "d0fd4d04-0db4-4306-bbc3-58cb20207a5f"
      },
      "source": [
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc39ebce5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDFcJkCRYggX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}